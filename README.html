<h1 id="auto-scaling-of-kubernetes-pods-using-keda-external-scaler">Auto Scaling of Kubernetes workloads using custom application metrics</h1>
<p>Orchestration platforms such as Kubernetes and OpenShift allow customers to achieve cost savings by providing on-demand scalable compute.
Customers can manually scale-out and scale-in their Kubernetes compute resources based on their need.</p>
<p>Autoscaling is the process of automatically and dynamically matching the compute resources to meet the performance requirements of a system.
As the workload grows, systems may need additional resources to maintain the necessary performance level or meet the growing demand.
If the demand slows down and the additional compute resources are no longer needed, customers can save cost by having an automatic service in place to scale-in unutilized compute resources.
Autoscaling is eliminating the need for manual human intervention in the scaling process, reducing the probability of incorrect resource allocation and saving cost by automatically allocating the exact compute power that is required.</p>
<p>In this post, we will show you how you can use KEDA to easily autoscale your Kubernetes or OpenShift pods based on your own custom in-memory application metrics.</p>
<p><a href="https://keda.sh/">KEDA</a> is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on custom application metrics that are external to Kubernetes.<br>
KEDA is introducing a new Kubernetes Custom Resource Definition called <a href="https://keda.sh/docs/2.9/concepts/scaling-deployments/#scaledobject-spec">ScaledObject</a>.
KEDA ScaledObject is used to define how KEDA should scale your application and what the triggers are.<br>
The external ScaledObject can be configured to periodically poll your application (over gRPC protocol) to get the name/value of the custom application metrics that should control the number of pods (<code>replicas</code>) for a specific Kubernetes Deployment/StatefulSet.<br><br>
This simple example is showing how to easilly add the related gRPC endpoints to your application to support such custom Auto Scaling.<br>
The gRPC API endpoints that should be added to your application are specified by the <a href="https://github.com/kubernetes-zookeeper/external-scaler-samples/blob/main/external-scaler-grpc/src/main/proto/externalscaler.proto">externalscaler.proto</a>.<br><br>
The following example is building and running <code>external-scaler-server</code> that is serving as the gRPC server of your application.<br>
The <code>external-scaler-server</code> is getting the custom metric from the backend <code>external-scaler-web</code> application.
Based on the periodic response from the <code>external-scaler-server</code>, KEDA external <a href="https://github.com/kubernetes-zookeeper/external-scaler-samples/blob/main/external-scaler-grpc/helm/external-scaler-server/templates/keda_scaled_object_deployment.yaml">ScaledObject</a> is automatically scaling the number of <code>worker</code> pods (<code>replicas</code>).<br></p>
<h3 id="-prerequisites"><a name="prerequisites"></a> Prerequisites</h3>
You will need the following to complete the steps in this post:
<ul>
<li><a href="https://kubernetes.io/docs/setup/">Kubernetes cluster</a></li>
<li><a href="https://helm.sh/docs/intro/install/">helm</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/#kubectl">kubectl</a></li>
</ul>
The following diagram shows the complete setup that we will walk through in this post
<p><img src="https://github.com/kubernetes-zookeeper/external-scaler-samples/assets/112578195/3cf7672f-17c0-4df2-a439-c1d5b85cd1b0" alt="KEDA-External-Scaler"></p>
<h2 id="keda-external-scaler-server">KEDA External Scaler Server Setup</h2>
<h3 id="-to-get-the-keda-external-scaler-server-example"><a name="to-get-the-examples"></a> Step 1: To get the KEDA External Scaler Server example</h3>
<ol>
<li>Clone the git repository of <code>external-scaler-samples</code></li>
<pre><code>git clone https://github.com/kubernetes-zookeeper/external-scaler-samples
cd ./external-scaler-samples
</code></pre>
</ol>
<h3 id="-to-build-the-keda-external-scaler-server-example"><a name="to-build-the-examples"></a> Step 2: To build the KEDA External Scaler Server example</h3>
<ol>
<p>
This build step is optional and can be skipped.<br><br>
The example is using the following 2 docker images 
<ul>
<li><code>kuberneteszookeeper/external-scaler-web</code></li>
<li><code>kuberneteszookeeper/external-scaler-server</code></li>
</ul>
<br>
These 2 docker images are already stored and available in <a href="https://hub.docker.com/u/kuberneteszookeeper">Docker hub</a>.<br>
If you would like to re-build these 2 docker images, you may execute the following step:
<br><br></p>
<li>From the top level <code>external-scaler-samples</code> directory:<br>
Download gradle 7.4 package and extract it to the gradle folder.</li>

<pre><code>wget -O gradle.zip https://services.gradle.org/distributions/gradle-7.4-bin.zip &amp;&amp; unzip gradle.zip &amp;&amp; /bin/mv gradle-7.4 gradle

./gradle/bin/gradle clean docker
</code></pre>

<p>This creates the docker images <code>kuberneteszookeeper/external-scaler-web</code> and <code>kuberneteszookeeper/external-scaler-server</code>.<br>
The <code>kuberneteszookeeper/external-scaler-web</code> and <code>kuberneteszookeeper/external-scaler-server</code> docker images should be accessible from the k8s cluster.
</ol>
</p>
<h3 id="-to-run-the-keda-external-scaler-server-example"><a name="to-run-the-examples"></a> Step 3: To run the KEDA External Scaler Server example</h3>
<ol start="1">
<li>To try the external scaler please run (from the top level <code>external-scaler-samples</code> directory):</li>
<pre><code>./external-scaler-grpc/helm/install_helm.sh
</code></pre>
<pre><code>./external-scaler-grpc/keda/install_keda.sh
</code></pre>
<pre><code>helm upgrade --install external-scaler-server ./external-scaler-grpc/helm/external-scaler-server/ --namespace external-scaler-server --create-namespace --values ./external-scaler-grpc/helm/external-scaler-server/values.yaml
</code></pre>
</ol>
<h2 id="autoscaling-in-action"><a name="autoscaling-in-action"></a> AutoScaling in Action</h2>
<p>The <a href="https://keda.sh/docs/latest/scalers/external/">external</a> KEDA ScaledObject is now periodically polling the external-scaler-server (over gRPC protocol) for the name/value of the metric.
Based on the value (and the target size for this metric), KEDA is scaling the related worker Deployment/StatefulSet.
In this specific example the <code>metricName</code> is <code>&quot;number_of_jobs&quot;</code> and its <code>targetSize</code> is <code>&quot;4&quot;</code>. That is, each worker pod should not run more than <code>&quot;4&quot;</code> application jobs.<br>
<br>
For example, when the <code>&quot;number_of_jobs&quot;</code> is <code>&quot;20&quot;</code>, KEDA should automatically scale the worker StatefulSet to (20 / 4 =) <code>&quot;5&quot;</code> pods.
<br>
In this specific example the polling interval (<code>pollingInterval</code>) is set to <code>10</code> seconds in helm <a href="https://github.com/kubernetes-zookeeper/external-scaler-samples/blob/main/external-scaler-grpc/helm/external-scaler-server/values.yaml">values.yaml</a>.
<br>
<br>
Letâ€™s put some load on the application by running the following command for creating <code>&quot;20&quot;</code> application jobs:</p>
<pre><code>kubectl exec -n external-scaler-server -it svc/external-scaler-web -- curl -X POST http://external-scaler-web.external-scaler-web.external-scaler-server.svc.cluster.local:8080/external-scaler-web/api/jobs?number_of_jobs=20
</code></pre>
<p>The <code>&quot;number_of_jobs&quot;</code> query parameter is controlling how many application jobs should be created.
<br>
You may now run the following and watch KEDA auto scaling in action:</p>
<pre><code>kubectl -n external-scaler-server get ScaledObject
kubectl -n external-scaler-server get HorizontalPodAutoScaler
kubectl -n external-scaler-server get pods -w
</code></pre>
<p>In a different terminal window, you may run the following command to view the gRPC application server log:</p>
<pre><code>kubectl -n external-scaler-server logs -f -l name=external-scaler-server
</code></pre>
<p>This example is scaling the number of worker pods (<code>replicas</code>) based on the value of an application metric in real time.<br>
<br>
The external-scaler-server pod is running a gRPC server that is responding to the periodic <code>getMetricSpec</code> and <code>getMetrics</code> requests that are generated by the external KEDA ScaledObject.<br>
<br>
The docker log of the external-scaler-server pod is showing the response for the periodic <code>getMetricSpec</code> and <code>getMetrics</code> requests that are generated by the external KEDA ScaledObject.
<br>
<br>
The <code>getMetricSpec</code> response specifies the name of custom application metric that should control the auto scaling and its target size.<br>
<br>
The periodic <code>getMetrics</code> response specifies the value of the custom application metric.<br>
<br>
The external KEDA ScaledObject is dividing the returned value of the custom application metric by its target size ( <code>&quot;4&quot;</code> ) - and set the number of worker pods (<code>replicas</code>) accordingly.<br>
For example, <code>metricValue: &quot;20&quot;</code> and <code>targetSize: &quot;4&quot;</code> will set the worker <code>replicas</code> to 5 ( <code>&quot;20&quot;</code> applications jobs require 5 worker pods ).
Each workder pod should not run more than <code>&quot;4&quot;</code> (<code>targetSize</code>) application jobs.
<br>
<br>
The worker pods are running each of the application jobs for a random period of time. When each job is completed, the worker is sending a rest call to the external scaler web for deleting the job (reducing the value of the <code>&quot;number_of_jobs&quot;</code> metric by 1).
<br>
<br>
In different terminal windows you may run the following commands and view the log of any of the worker pods (running the application jobs) or the external scaler web (managing the number of jobs):</p>
<pre><code>kubectl logs -n external-scaler-server -f worker-0
</code></pre>
<pre><code>kubectl logs -n external-scaler-server -f -l name=external-scaler-web
</code></pre>
<p>The <code>targetSize</code> (<code>numberOfJobsPerServer</code>), <code>minimumJobDuration</code> and <code>maximumJobDuration</code> are controlled by the following helm <a href="https://github.com/kubernetes-zookeeper/external-scaler-samples/blob/main/external-scaler-grpc/helm/external-scaler-server/values.yaml">values.yaml</a>:</p>
<pre><code>externalScalerServer:
  numberOfJobsPerServer: &quot;4&quot;
  minimumJobDuration: &quot;10&quot;
  maximumJobDuration: &quot;20&quot;
</code></pre>
<h2 id="clean-up">Clean-up</h2>
<p>Use the following commands to delete resources created during this post:</p>
<pre><code>helm uninstall external-scaler-server --namespace external-scaler-server
</code></pre>
<pre><code>helm uninstall keda --namespace keda
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li>The documentation of the KEDA External Scalers is here: <a href="https://keda.sh/docs/latest/concepts/external-scalers/">External scalers</a></li>
<li>The source code of the External Scaler Server sample is here: <a href="https://github.com/kubernetes-zookeeper/external-scaler-samples/tree/main/external-scaler-grpc/src/main/java/io/grpc/examples/externalscaler">External scaler</a></li>

<li>For more information, refer to KEDA
<a href="https://keda.sh/">keda</a>.</li>
</ul>
